Amazon S3:
---------

Section introduction:
--------------------
• Amazon S3 is one of the main building blocks of AWS
• It’s advertised as ”infinitely scaling” storage
• Many websites use Amazon S3 as a backbone
• Many AWS services use Amazon S3 as an integration as well
• We’ll have a step-by-step approach to S3

Amazon S3 Use cases:
-------------------
• Backup and storage
• Disaster Recovery
• Archive
• Hybrid Cloud storage
• Application hosting
• Media hosting
• Data lakes & big data analytics
• Software delivery (updates)
• Static website
• Two major usecases of S3:
	1. Nasdaq stores 7 years of data into S3 Glacier
	2. Sysco runs analytics on its data and gain business insights

Amazon S3 - Buckets:
-------------------
• Amazon S3 allows people to store objects (files) in “buckets” (directories)
• Buckets must have a globally unique name (across all regions all accounts)
• Buckets are defined at the region level
• S3 looks like a global service but buckets are created in a region
• Naming convention
	• No uppercase, No underscore
	• 3-63 characters long
	• Not an IP
	• Must start with lowercase letter or number
	• Must NOT start with the prefix xn--
	• Must NOT end with the suffix -s3alias
	
Amazon S3 - Objects:
-------------------
• Objects (files) have a Key
• The key is the FULL path of the file:
	• s3://my-bucket/my_file.txt
	• s3://my-bucket/my_folder1/another_folder/my_file.txt
• The key is composed of prefix + object name
	• s3://my-bucket/my_folder1/another_folder/my_file.txt
• There’s no concept of “directories” within buckets (although the UI will trick you to think otherwise). But anything and everything in Amazon S3 is actually a Key.
• Just keys with very long names that contain slashes (“/”)
• Object values are the content of the body [Whatever is uploaded on Amazon S3]:
	• Max. Object Size is 5TB (5000GB)
	• If uploading more than 5GB, must use “multi-part upload”
• Metadata (list of text key / value pairs – system or user metadata)
• Tags (Unicode key / value pair – up to 10) – useful for security / lifecycle
• Version ID (if versioning is enabled)
• Objects are constst of Key, Value and Metadata.
• Objects are stored in Buckets in Amazon S3 (Not Files, Folders or Bins).

-----------------------------------

Amazon S3 – Security:
--------------------
• User-Based
	• IAM Policies – which API calls should be allowed for a specific user from IAM
• Resource-Based
	• Bucket Policies – bucket wide rules from the S3 console - allows cross account
	• Object Access Control List (ACL) – finer grain (can be disabled)
	• Bucket Access Control List (ACL) – less common (can be disabled)
• Note: an IAM principal can access an S3 object if
	• The user IAM permissions ALLOW it OR the resource policy ALLOWS it
	• AND there’s no explicit DENY
• Encryption: encrypt objects in Amazon S3 using encryption keys

S3 Bucket Policies:
------------------
{
	"Version": "2012-10-17",
	"Statement": [
		{
			"Sid": "PublicRead",
			"Effect": "Allow",
			"Principal": "*",
			"Action": [
				"s3:GetObject"
			],
			"Resource": ["arn:aws:s3:::examplebucket/*"]
		}
	]
}

• JSON based policies
	• Resources: buckets and objects
	• Effect: Allow / Deny
	• Actions: Set of API to Allow or Deny
	• Principal: The account or user to apply the policy to
• Use S3 bucket for policy to:
	• Grant public access to the bucket
	• Force objects to be encrypted at upload
	• Grant access to another account (Cross Account)
• Access Bucket
	• If an external user wants to access the S3 bucket then we need to use Bucket Policy to allow public access [Public Access - Use Bucket Policy]
	• If an IAM User wants to access the S3 bucket then we need to use IAM Policy [User Access to S3 - IAM permissions]
	• If an EC2 instance wants to access the S3 bucket we need to use EC2 Instance Role (IAM Roles) [EC2 Instance access - Use IAM Roles]
	• If an IAM User of another AWS account wants to access the S3 bucket of our account we need to use S3 Bucket Policy to allow Cross-Account [Cross-Account Access - Use Bucket Policy]
	
Bucket settings for Block Public Access:
---------------------------------------
• These settings were created to prevent company data leaks
• If you know your bucket should never be public, leave these on:
Block all public access [On]:
1. Block public access to buckets and objects granted through new access control lists (ACLs) [On]
2. Block public access to buckets and objects granted through any access control lists (ACLs) [On]
3. Block public access to buckets and objects granted through new public bucket or access point policies [On]
4. Block public and cross-account access to buckets and objects through any public bucket or access point policies [On]
• If you know your bucket should never be public, then these can be set at the account level

-----------------------------------

Amazon S3 – Static Website Hosting:
----------------------------------
• S3 can host static websites and have them accessible on the Internet
• The website URL will be (depending on the region)
	• http://<bucket-name>.s3-website-<aws-region>.amazonaws.com
	OR
	• http://<bucket-name>.s3-website.<aws-region>.amazonaws.com
• If you get a 403 Forbidden error, make sure the bucket policy allows public reads!

-----------------------------------

Amazon S3 -Versioning:
---------------------
• You can version your files in Amazon S3
• It is enabled at the bucket level
• Same key overwrite will change the “version”: 1, 2, 3….
• It is best practice to version your buckets
	• Protect against unintended deletes (ability to restore a version)
	• Easy roll back to previous version
• Notes:
	• Any file that is not versioned prior to enabling versioning will have version “null”
	• Suspending versioning does not delete the previous versions
	
-----------------------------------

Amazon S3 – Replication (CRR & SRR):
-----------------------------------
• Must enable Versioning in source and destination buckets
• Cross-Region Replication (CRR)
• Same-Region Replication (SRR)
• Buckets can be in different AWS accounts
• Copying is asynchronous [Replication happens in the Background]
• Must give proper IAM permissions to S3
• Use cases:
	• CRR – compliance, lower latency access, replication across accounts
	• SRR – log aggregation, live replication between production and test accounts
		
-----------------------------------

S3 Storage Classes:
------------------
• Amazon S3 Standard - General Purpose
• Amazon S3 Standard-Infrequent Access (IA)
• Amazon S3 One Zone-Infrequent Access
• Amazon S3 Glacier Instant Retrieval
• Amazon S3 Glacier Flexible Retrieval
• Amazon S3 Glacier Deep Archive
• Amazon S3 Intelligent Tiering
• Can move between classes manually or using S3 Lifecycle configurations [Lifecycle Rules can be used to define when S3 objects should be transitioned to another storage class or when objects should be deleted after some time.]

S3 Durability and Availability:
------------------------------
• Durability:
	• High durability (99.999999999%, 11 9’s) of objects across multiple AZ
	• If you store 10,000,000 objects with Amazon S3, you can on average expect to incur a loss of a single object once every 10,000 years
	• Same for all storage classes
• Availability:
	• Measures how readily available a service is
	• Varies depending on storage class
	• Example: S3 standard has 99.99% availability = not available 53 minutes a year

S3 Standard – General Purpose:
-----------------------------
• 99.99% Availability
• Used for frequently accessed data
• Low latency and high throughput
• Sustain 2 concurrent facility failures
• Use Cases: Big Data analytics, mobile & gaming applications, content distribution…

S3 Storage Classes – Infrequent Access:
--------------------------------------
• For data that is less frequently accessed, but requires rapid access when needed
• Lower cost than S3 Standard [But a cost on retrieval]
• Amazon S3 Standard-Infrequent Access (S3 Standard-IA)
	• 99.9% Availability
	• Use cases: Disaster Recovery, backups
• Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA)
	• High durability (99.999999999%) in a single AZ; data lost when AZ is destroyed
	• 99.5% Availability
	• Use Cases: Storing secondary backup copies of on-premise data, or data you can recreate
	
Amazon S3 Glacier Storage Classes:
---------------------------------
• Low-cost object storage meant for archiving / backup
• Pricing: price for storage + object retrieval cost
• Amazon S3 Glacier Instant Retrieval
	• Millisecond retrieval, great for data accessed once a quarter
	• Minimum storage duration of 90 days
	• So, this is backup but you want to access it withing milliseconds.
• Amazon S3 Glacier Flexible Retrieval (formerly Amazon S3 Glacier):
	• Expedited (1 to 5 minutes), Standard (3 to 5 hours), Bulk (5 to 12 hours) – free
	• Minimum storage duration of 90 days
	• Instant means you retrieve your data instantly and here flexible means you are willing to wait a flexible amount of time to retrieve the data.
• Amazon S3 Glacier Deep Archive – for long term storage:
	• Standard (12 hours), Bulk (48 hours)
	• Minimum storage duration of 180 days
	• Amazon Glacier Deep Archive is the most cost-effective option if you want to archive data and do not have a retrieval time requirement.
	
S3 Intelligent-Tiering:
----------------------
• Moves objects automatically between Access Tiers based on usage
• Small monthly monitoring and auto-tiering fee
• There are no retrieval charges in S3 Intelligent-Tiering
• Frequent Access tier (automatic): default tier
• Infrequent Access tier (automatic): objects not accessed for 30 days
• Archive Instant Access tier (automatic): objects not accessed for 90 days
• Archive Access tier (optional): configurable from 90 days to 700+ days
• Deep Archive Access tier (optional): config. from 180 days to 700+ days
		
-----------------------------------

S3 Encryption:
-------------
- Server-Side Encryption (Default): 
	o Whenever you create a bucket or whenever you upoad an object, it will be encrypted. 
	o Here, the server is doing the encryption for security purposes when the object arrives in the bucket. That's why it is called Server-Side Encryption.
	o By default Server-Side Encryption is always on.
- Client-Side Encryption:
	o In this case the User Encrypts the file before uploading it. So the lock is done by the user, and then put it in the bucket. That's why it is called Client-Side Encryption.
		
-----------------------------------

Shared Responsibility Model for S3:
----------------------------------
- AWS:
	• Infrastructure (global security, durability, availability, sustain concurrent loss of data in two facilities)
	• Configuration and vulnerability analysis
	• Compliance validation

- You:
	• S3 Versioning
	• S3 Bucket Policies
	• S3 Replication Setup
	• Logging and Monitoring as they are optional
	• S3 Storage Classes
	• Data encryption at rest and in transit
		
-----------------------------------

AWS Snow Family:
---------------
• Highly-secure, portable devices to collect and process data at the edge, and migrate data into and out of AWS
• Data migration: Snowcone, Snowball Edge, Snowmobile
• Edge computing: Snowcone, Snowball Edge

Data Migrations with AWS Snow Family:
------------------------------------
Time to Transfer:
			100 Mbps 	1Gbps 		10Gbps
10 TB 		12 days 	30 hours 	3 hours
100 TB 		124 days 	12 days 	30 hours
1 PB 		3 years 	124 days 	12 days

Challenges:
• Limited connectivity
• Limited bandwidth
• High network cost
• Shared bandwidth (can’t maximize the line)
• Connection stability

- AWS Snow Family: offline devices to perform data migrations. If it takes more than a week to transfer over the network, use Snowball devices!

Snowball Edge (for data transfers):
----------------------------------
• Physical data transport solution: move TBs or PBs of data in or out of AWS
• Alternative to moving data over the network (and paying network fees)
• Pay per data transfer job 
• Provide block storage and Amazon S3-compatible object storage
• Snowball Edge Storage Optimized 
	• 80 TB of HDD capacity for block volume and S3 compatible object storage
• Snowball Edge Compute Optimized 
	• 42 TB of HDD capacity for block volume and S3 compatible object storage
• Use cases: large data cloud migrations, Do decommission, disaster recovery
• For Snowball Edge, we can do Storage Clustering, so we can put upto 15 Snowball Edges together to increase the storage size.
• Snowball Edge is best-suited to move petabytes of data and offers computing capabilities. Be careful, it's recommended to use a fleet of Snowballs to move less than 10PBs of data. Over this quantity, it's better-suited to use Snowmobile.
• Snowball Edge Compute Optimized provides powerful computing resources for higher performance workloads such as machine learning, full motion video analysis, analytics, and local computing stacks. Despite providing local computing capacity, AWS Snowball Edge Compute Optimized devices are not best-suited for data transfer, but instead the high performance workloads mentioned above.
• Snowball Edge Storage Optimized devices are well suited for large-scale data migrations and recurring transfer workflows, as well as local computing with higher capacity needs.

AWS Snowcone & Snowcone SSD:
---------------------------
• Small, portable computing, anywhere, rugged & secure, withstands harsh environments 
• Light (4.5 pounds, 2.1 kg) 
• Device used for edge computing, storage, and data transfer
• Snowcone – 8 TB of HDD Storage
• Snowcone SSD – 14 TB of SSD Storage
• Use Snowcone where Snowball does not fit (space - constrained environment)
• Must provide your own battery / cables 
• Can be sent back to AWS offline, or connect it to internet and use AWS DataSync to send data
• AWS Snowcone is a small, portable, rugged, and secure edge computing and data transfer device. It provides up to 8 TB of usable storage

AWS Snowmobile:
--------------
• Transfer exabytes of data (1 EB = 1,000 PB = 1,000,000 TBs)
• Each Snowmobile has 100 PB of capacity (use multiple in parallel)
• High security: temperature controlled, GPS, 24/7 video surveillance
• Better than Snowball if you transfer more than 10 PB

AWS Snow Family for Data Migrations:
-----------------------------------
						Snowcone & Snowcone SSD 			Snowball Edge Storage Optimized			Snowmobile
Storage Capacity 		8 TB HDD & 14 TB SSD				80 TB usable 							< 100 PB
Migration Size 			Up to 24 TB, online and offline     Up to petabytes, offline				Up to exabytes, offline
DataSync agent 			Pre-installed						
Storage Clustering 											Up to 15 nodes

Snow Family – Usage Process:
---------------------------
1. Request Snowball devices from the AWS console for delivery
2. Install the snowball client / AWS OpsHub on your servers
3. Connect the snowball to your servers and copy files using the client
4. Ship back the device when you’re done (goes to the right AWS facility)
5. Data will be loaded into an S3 bucket
6. Snowball is completely wiped

What is Edge Computing?
----------------------
• Process data while it’s being created on an edge location
	• A edge location that has no connection to the cloud (or internet): A truck on the road, a ship on the sea, a mining station underground...
• These locations may have
	• Limited / no internet access
	• Limited / no easy access to computing power
• We setup a Snowball Edge / Snowcone device to do edge computing
• Use cases of Edge Computing:
	• Preprocess data
	• Machine learning at the edge
	• Transcoding media streams
• Eventually (if need be) we can ship back the device to AWS (for transferring data for example)

Snow Family – Edge Computing:
----------------------------
• Snowcone & Snowcone SSD (smaller)
	• 2 CPUs, 4 GB of memory, wired or wireless access
	• USB-C power using a cord or the optional battery
• Snowball Edge – Compute Optimized
	• 52 vCPUs, 208 GiB of RAM
	• Optional GPU (useful for video processing or machine learning)
	• 42 TB usable storage
• Snowball Edge – Storage Optimized
	• Up to 40 vCPUs, 80 GiB of RAM
	• Object storage clustering available
• All: Can run EC2 Instances & AWS Lambda functions (using AWS IoT Greengrass)
• Long-term deployment options: 1 and/or 3 years discounted pricing

AWS OpsHub:
----------
• Historically, to use Snow Family devices, you needed a CLI (Command Line Interface tool)
• Today, you can use AWS OpsHub (a software you install on your computer / laptop) to manage your Snow Family Device
	• Unlocking and configuring single or clustered devices
	• Transferring files
	• Launching and managing instances running on Snow Family Devices
	• Monitor device metrics (storage capacity, active instances on your device)
	• Launch compatible AWS services on your devices (ex: Amazon EC2 instances, AWS DataSync, Network File System (NFS))

-----------------------------------

Hybrid Cloud for Storage:
------------------------
• AWS is pushing for ”hybrid cloud”
	• Part of your infrastructure is on-premises
	• Part of your infrastructure is on the cloud
• This can be due to
	• Long cloud migrations
	• Security requirements
	• Compliance requirements
	• IT strategy
• S3 is a proprietary storage technology (unlike EFS / NFS), so how do you expose the S3 data on-premise?
	• AWS Storage Gateway!

AWS Storage Cloud Native Options:
--------------------------------
- Block:
	o Amazon EBS
	o EC2 Instance Store
- File:
	o Amazon EFS
- Object:
	o Amazon S3
	o Glacier
	
AWS Storage Gateway:
-------------------
• Bridge between on-premise data and cloud data in S3
• Hybrid storage service to allow on- premises to seamlessly use the AWS Cloud
• Use cases: disaster recovery, backup & restore, tiered storage
• Types of Storage Gateway:
	• File Gateway
	• Volume Gateway
	• Tape Gateway
• No need to know the types at the exam
• Storage Gateway allows you to bridge whatever happens on-premises directly into the AWS cloud. So behind the scene, the Storage Gateway will be using Amazon EBS, Amazon S3, and Glacier.
• So from a certified cloud petitioner perspective, the Storage Gateway is a way for you to bridge your file systems and your storage on-premises into the cloud to leverage best of both worlds

-----------------------------------

Amazon S3 – Summary:
-------------------
• Buckets vs Objects: global unique name, tied to a region
• S3 security: IAM policy, S3 Bucket Policy (public access), S3 Encryption
• S3 Websites: host a static website on Amazon S3
• S3 Versioning: multiple versions for files, prevent accidental deletes
• S3 Replication: same-region or cross-region, must enable versioning
• S3 Storage Classes: Standard, IA, 1Z-IA, Intelligent, Glacier (Instant, Flexible, Deep)
• Snow Family: import data onto S3 through a physical device, edge computing
• OpsHub: desktop application to manage Snow Family devices
• Storage Gateway: hybrid solution to extend on-premises storage to S3

-----------------------------------